{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_k4p4K7kIKR",
        "outputId": "d5f66396-5947-4ad6-8d49-35c7eaf57c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMqC5Uw2gaka"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCOM583qcEp5"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Tleulzkmo-"
      },
      "outputs": [],
      "source": [
        "task_path = '/content/drive/My Drive/Supernova-NLP/intent classification/intent classification'\n",
        "max_length = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_metric\n",
        "from datasets import DatasetDict, Dataset, ClassLabel\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYGlTdwPQ3Ym"
      },
      "source": [
        "[link text](https://)Read sentences from excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxTYPUYBj7No"
      },
      "outputs": [],
      "source": [
        "df_sentences = pd.read_csv(f'{task_path}/sentiment-data.csv')[[\"INTENT\", \"SENTENCES\"]]\n",
        "df_sentences = df_sentences.loc[pd.notnull(df_sentences['INTENT'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICuqWAGpkdda"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.subplots()\n",
        "ax.hist(df_sentences['INTENT'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdPFCSMyRs4i"
      },
      "source": [
        "Make datasets.Dataset object and split into two sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIgX7FRYs1CI"
      },
      "outputs": [],
      "source": [
        "classes = list(pd.unique(df_sentences['INTENT']))\n",
        "classLabel = ClassLabel(num_classes=len(classes), names=classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvpOPwsQkeLU"
      },
      "outputs": [],
      "source": [
        "with open(f'{task_path}/models/classLabel.pickle', 'wb') as handle:\n",
        "    pickle.dump(classLabel, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSZZvXivj8Er"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_dict({'sentence':df_sentences['SENTENCES'], 'label':classLabel.str2int(df_sentences['INTENT']), 'idx':df_sentences.index})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMwlUQU_IXTe"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.class_encode_column('label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1AynHApA1Ht"
      },
      "outputs": [],
      "source": [
        "train_size = 0.9\n",
        "dataset = dataset.train_test_split(train_size=train_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYDNCX9vcpUI"
      },
      "outputs": [],
      "source": [
        "dataset['train'], dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwev5Hotjte1"
      },
      "outputs": [],
      "source": [
        "dataset['test'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnjDIuQ3IrI-"
      },
      "source": [
        "Load Metric. The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KRV6XpwuiDS"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o4rUteaIrI_"
      },
      "outputs": [],
      "source": [
        "metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OH3j00VSBJX"
      },
      "source": [
        "Import tokenizer for XLM-Roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu_r0moA7GY-"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')#, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on one sentence or a pair of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "outputs": [],
      "source": [
        "#tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\", truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbP_PdFSSLhi"
      },
      "source": [
        "Do all preproccessing with this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQTwJ-PDRf1M"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    # All sentences will be padded ot truncated to max_length\n",
        "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(preprocess_function, batched=True, load_from_cache_file=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKZXGT-ym970"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format(type = 'torch', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovxAjpJS2eG"
      },
      "source": [
        "Import Model and Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaConfig\n",
        "\n",
        "config = XLMRobertaConfig.from_pretrained('xlm-roberta-base')\n",
        "config.num_labels = classLabel.num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srx1-OFDtbdx"
      },
      "outputs": [],
      "source": [
        "model = XLMRobertaForSequenceClassification(config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGORUEd3pvRu"
      },
      "outputs": [],
      "source": [
        "! pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khUJlK2yppQc"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "metric_name = \"accuracy\"\n",
        "batch_size = 16\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{task_path}/models\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=0.05,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    report_to='wandb'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLTrTTyMZJCz"
      },
      "outputs": [],
      "source": [
        "!wandb login --relogin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2dwvB6do0yB"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"projecName\", entity=\"username\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKASz-2vIrJi"
      },
      "source": [
        "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUcBkX8IrJi"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWGUpPnZBGBr"
      },
      "outputs": [],
      "source": [
        "\n",
        " trainer.save_model(task_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k8ge1L1IrJk"
      },
      "source": [
        "## Hyperparameter search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNfajuw_IrJl"
      },
      "source": [
        "The `Trainer` supports hyperparameter search using [optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/). For this last section you will need either of those libraries installed, just uncomment the line you want on the next cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUdakNBhIrJl"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# ! pip install optuna\n",
        "# ! pip install ray[tune]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttfT0CqaIrJm"
      },
      "source": [
        "During hyperparameter search, the `Trainer` will run several trainings, so it needs to have the model defined via a function (so it can be reinitialized at each new run) instead of just having it passed. We jsut use the same function as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sgjdLKcIrJm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def model_init():\n",
        "    config = XLMRobertaConfig.from_pretrained('xlm-roberta-base')\n",
        "    config.num_labels = classLabel.num_classes\n",
        "    return XLMRobertaForSequenceClassification(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMXfVJO4IrJo"
      },
      "source": [
        "And we can instantiate our `Trainer` like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71pt6N0eIrJo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset=encoded_dataset[\"train\"].shard(index=1, num_shards=5) ,\n",
        "    eval_dataset=encoded_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQxrzFP4IrJq"
      },
      "source": [
        "The method we call this time is `hyperparameter_search`. Note that it can take a long time to run on the full dataset for some of the tasks. You can try to find some good hyperparameter on a portion of the training dataset by replacing the `train_dataset` line above by:\n",
        "```python\n",
        "train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10) \n",
        "```\n",
        "for 1/10th of the dataset. Then you can run a full training on the best hyperparameters picked by the search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NboJ7kDOIrJq"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUTD72qCIrJs"
      },
      "source": [
        "The `hyperparameter_search` method returns a `BestRun` objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psi4JymeIrJs"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "best_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFdjWbRIIrJu"
      },
      "source": [
        "You can customize the objective to maximize by passing along a `compute_objective` function to the `hyperparameter_search` method, and you can customize the search space by passing a `hp_space` argument to `hyperparameter_search`. See this [forum post](https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10) for some examples.\n",
        "\n",
        "To reproduce the best training, just set the hyperparameters in your `TrainingArgument` before creating a `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsJ6sqdGIrJu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "for n, v in best_run.hyperparameters.items():\n",
        "    setattr(trainer.args, n, v)\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCMDT4kU4yg1"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHcD4wKncEqI"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzWT6DT5raIX"
      },
      "source": [
        "Load model, tokenizer and label mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVfoI8bI9338"
      },
      "outputs": [],
      "source": [
        "model_folder = f'{task_path}'\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(model_folder)\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(model_folder)\n",
        "with open(f'{task_path}/models/classLabel.pickle', 'rb') as handle:\n",
        "    classLabel = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_7GnXyGkRXw"
      },
      "outputs": [],
      "source": [
        "dataset['test'][540]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FITVD00nIiZ"
      },
      "source": [
        "Enter sentence here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoqWawpiJcK8"
      },
      "outputs": [],
      "source": [
        "demo_sentence = \"6 სექტემებრის მონაცმებით 2514 ადმიანი დაინფიცირდა\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIpirBCsrgUp"
      },
      "source": [
        "Tokenize sentence and predict label with model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM3TQsy7N0kk"
      },
      "outputs": [],
      "source": [
        "tokenized = tokenizer(demo_sentence, padding='max_length', truncation=True, max_length=max_length)\n",
        "input_ids = torch.LongTensor([tokenized['input_ids']])\n",
        "attention_mask = torch.FloatTensor([tokenized['attention_mask']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NOuoTbAN9Wn"
      },
      "outputs": [],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeT5MbLDOCSu"
      },
      "outputs": [],
      "source": [
        "attention_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8tQ72bROFhl"
      },
      "outputs": [],
      "source": [
        "outputs = []\n",
        "for i,j in input_ids,attention_mask:\n",
        "\n",
        "  label_idx = model.forward(i,j).logits.argmax()\n",
        "  outputs.append(label_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj1gOCA0toPw"
      },
      "outputs": [],
      "source": [
        "label_idx = model.forward(input_ids,attention_mask).logits.argmax()\n",
        "label_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpHtDVPkmVM8"
      },
      "outputs": [],
      "source": [
        "for i in\n",
        "  label = classLabel.int2str([label_idx])\n",
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WJp-2IyuMTm"
      },
      "outputs": [],
      "source": [
        "  label = classLabel.int2str([label_idx])\n",
        "label"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
